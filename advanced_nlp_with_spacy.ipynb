{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b3f960",
   "metadata": {},
   "source": [
    "# Chapter 1: Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3a32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63acdf",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "Let's get started and try out spaCy! In this exercise, you'll be able to try out some of the 45+ available languages.\n",
    "\n",
    "This course introduces a lot of new concepts, so if you ever need a quick refresher, download the spaCy Cheat Sheet and keep it handy!\n",
    "\n",
    "#### Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Import the English class from spacy.lang.en and create the nlp object.\n",
    "Create a doc and print its text.\n",
    "\n",
    "Show Answer (-35 XP)\n",
    "2\n",
    "Import the German class from spacy.lang.de and create the nlp object.\n",
    "Create a doc and print its text.\n",
    "3\n",
    "Import the Spanish class from spacy.lang.es and create the nlp object.\n",
    "Create a doc and print its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d2c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77210a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Grüße!\n"
     ]
    }
   ],
   "source": [
    "# Import the German language class\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = German()\n",
    "\n",
    "# Process a text (this is German for: \"Kind regards!\")\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2761e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "# Import the Spanish language class\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = Spanish()\n",
    "\n",
    "# Process a text (this is Spanish for: \"How are you?\")\n",
    "doc = nlp(\"¿Cómo estás?\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5b2fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de2c6f",
   "metadata": {},
   "source": [
    "### Documents, spans and tokens\n",
    "When you call nlp on a string, spaCy first tokenizes the text and creates a document object. In this exercise, you'll learn more about the Doc, as well as its views Token and Span.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Import the English language class and create the nlp object.\n",
    "Process the text and instantiate a Doc object in the variable doc.\n",
    "Select the first token of the Doc and print its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb89081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef6a1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:-1]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4beb158",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac7cbf",
   "metadata": {},
   "source": [
    "### Lexical attributes\n",
    "\n",
    "In this example, you'll use spaCy's Doc and Token objects, and lexical attributes to find percentages in a text. You'll be looking for two subsequent tokens: a number and a percent sign. The English nlp object has already been created.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Use the like_num token attribute to check whether a token in the doc resembles a number.\n",
    "Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.\n",
    "Check whether the next token's text attribute is a percent sign \"%\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36bf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e7cfa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd260a9",
   "metadata": {},
   "source": [
    "### Loading models\n",
    "\n",
    "Let's start by loading a model. spacy is already imported.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Use spacy.load to load the small English model 'en_core_web_sm'.\n",
    "Process the text and print the document text.\n",
    "\n",
    "Take Hint (-15 XP)\n",
    "2\n",
    "Use spacy.load to load the small German model 'de_core_news_sm'.\n",
    "Process the text and print the document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a02ebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: jinja2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.22.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: setuptools in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (62.3.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.3.0/de_core_news_sm-3.3.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from de-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.22.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (62.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: jinja2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm \n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72dd1fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\n"
     ]
    }
   ],
   "source": [
    "# Load the 'en_core_web_sm' model – spaCy is already imported\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13246dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Als erstes Unternehmen der Börsengeschichte hat Apple einen Marktwert von einer Billion US-Dollar erreicht\n"
     ]
    }
   ],
   "source": [
    "# Load the 'de_core_news_sm' model – spaCy is already imported\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "text = \"Als erstes Unternehmen der Börsengeschichte hat Apple einen Marktwert von einer Billion US-Dollar erreicht\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350589a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675df1c",
   "metadata": {},
   "source": [
    "### Predicting linguistic annotations\n",
    "\n",
    "You'll now get to try one of spaCy's pre-trained model packages and see its predictions in action. Feel free to try it out on your own text! The small English model is already available as the variable nlp.\n",
    "\n",
    "To find out what a tag or label means, you can call spacy.explain in the IPython shell. For example: spacy.explain('PROPN') or spacy.explain('GPE').\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Process the text with the nlp object and create a doc.\n",
    "For each token, print the token text, the token's .pos_ (part-of-speech tag) and the token's .dep_ (dependency label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b172446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PROPN     pnc       \n",
      "’s          PROPN     ROOT      \n",
      "official    ADV       mnr       \n",
      ":           PUNCT     punct     \n",
      "Apple       X         pnc       \n",
      "is          X         uc        \n",
      "the         X         uc        \n",
      "first       X         pnc       \n",
      "U.S.        X         app       \n",
      "public      X         punct     \n",
      "company     X         uc        \n",
      "to          X         uc        \n",
      "reach       X         uc        \n",
      "a           PROPN     pnc       \n",
      "$           PROPN     ag        \n",
      "1           NUM       pnc       \n",
      "trillion    NOUN      ROOT      \n",
      "market      PROPN     pnc       \n",
      "value       PROPN     nk        \n"
     ]
    }
   ],
   "source": [
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e371f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ab9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872038f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b06f3",
   "metadata": {},
   "source": [
    "### Predicting named entities in context\n",
    "\n",
    "Models are statistical and not always right. Whether their predictions are correct depends on the training data and the text you're processing. Let's take a look at an example. The small English model is available as the variable nlp.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Process the text with the nlp object.\n",
    "Iterate over the entities with the iterator ent and print the entity text and label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1765e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n"
     ]
    }
   ],
   "source": [
    "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a1cffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print('Missing entity:', iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0cfb75",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb1615",
   "metadata": {},
   "source": [
    "### Using the Matcher\n",
    "\n",
    "Let's try spaCy's rule-based Matcher. You'll be using the example from the previous exercise and write a pattern that can match the phrase \"iPhone X\" in the text. The nlp object and a processed doc are already available.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "Import the Matcher from spacy.matcher.\n",
    "Initialize it with the nlp object's shared vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06b28c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "# Import the Matcher and initialize it with the shared vocabulary\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('IPHONE_X_PATTERN', [pattern], on_match = None)\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c7eb1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b7314",
   "metadata": {},
   "source": [
    "Below is an example of regex in Spacy's matcher. Here is the [link](https://stackoverflow.com/questions/63280191/how-can-i-use-spacy-matcher-or-phrasematcher-class-for-the-extracting-the-sequ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "123d3e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6184782843368750095 ColoredAnimals 2 4 red fox\n",
      "6184782843368750095 ColoredAnimals 6 8 Black Hare\n",
      "6184782843368750095 ColoredAnimals 12 14 whItE sQuirrel\n",
      "6184782843368750095 ColoredAnimals 15 17 brown wolf\n",
      "6184782843368750095 ColoredAnimals 18 20 gray bear\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "colors=['red','gray','black','white','brown']\n",
    "animals=['fox','bear','hare','squirrel','wolf']\n",
    "pattern = [\n",
    "   {'TEXT': {\"REGEX\": \n",
    "             fr\"(?i)^({'|'.join(colors)})$\"}},\n",
    "   {'TEXT': {\"REGEX\": fr\"(?i)^({'|'.join(animals)})$\"}}\n",
    "]\n",
    "matcher.add(\"ColoredAnimals\", [pattern], on_match = None)\n",
    "\n",
    "doc = nlp(\"Hello, red fox! Hello Black Hare! What's up whItE sQuirrel, brown wolf and gray bear!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4395c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c31a8",
   "metadata": {},
   "source": [
    "### Writing match patterns\n",
    "\n",
    "In this exercise, you'll practice writing more complex match patterns using different token attributes and operators. A matcher is already initialized and available as the variable matcher.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "Write one pattern that only matches mentions of the full iOS versions: \"iOS 7\", \"iOS 11\" and \"iOS 10\".\n",
    "\n",
    "__2__\n",
    "\n",
    "Write one pattern that only matches forms of \"download\" (tokens with the lemma \"download\"), followed by a token with the part-of-speech tag 'PROPN' (proper noun).\n",
    "\n",
    "__3__\n",
    "\n",
    "Write one pattern that matches adjectives ('ADJ') followed by one or two 'NOUN's (one noun and one optional noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67cddd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', [pattern], on_match = None)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2e5236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when \\\n",
    "I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I \\\n",
    "used the default program to unpack it... do I also need to download Winzip?\")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('DOWNLOAD_THINGS_PATTERN', [pattern], on_match = None)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76b416e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Features of the app include a beautiful design, smart search, \\\n",
    "automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', [pattern], on_match = None)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0ff82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95f323",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b10310",
   "metadata": {},
   "source": [
    "### Strings to hashes\n",
    "\n",
    "The nlp object has already been created for you.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Look up the string \"cat\" in nlp.vocab.strings to get the hash.\n",
    "Look up the hash to get back the string.\n",
    "\n",
    "2\n",
    "\n",
    "Look up the string label \"PERSON\" in nlp.vocab.strings to get the hash.\n",
    "Look up the hash to get back the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab1d4ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings['cat']\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c5f9d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings['PERSON']\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d51dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168eb96d",
   "metadata": {},
   "source": [
    "### Vocab, hashes and lexemes\n",
    "\n",
    "Why does this code throw an error?\n",
    "\n",
    "```\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = English()\n",
    "nlp_de = German()\n",
    "```\n",
    "\n",
    "```\n",
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings['Bowie']\n",
    "print(bowie_id)\n",
    "\n",
    "# Look up the ID for 'Bowie' in the vocab\n",
    "print(nlp_de.vocab.strings[bowie_id])\n",
    "```\n",
    "The English language class is already available as the nlp object.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "The string 'Bowie' isn't present in the German vocab, so the hash can't be resolved in the string store.\n",
    "\n",
    "'Bowie' is not a regular word in the English or German dictionary, so it can't be hashed.\n",
    "\n",
    "nlp_de is not a valid name. The vocab can only be shared if the nlp objects have the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70fdaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644858412616767388\n",
      "\n",
      "[E018] Can't retrieve string for hash '2644858412616767388'. This usually refers to an issue with the `Vocab` or `StringStore`.'\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = English()\n",
    "nlp_de = German()\n",
    "\n",
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings['Bowie']\n",
    "print(bowie_id)\n",
    "print('')\n",
    "\n",
    "# Look up the ID for 'Bowie' in the vocab\n",
    "try:\n",
    "    print(nlp_de.vocab.strings[bowie_id])\n",
    "except KeyError: # created try except block so Jupyter Notebook would not stop working.\n",
    "    print(\"[E018] Can't retrieve string for hash '2644858412616767388'. This usually refers to an issue with the `Vocab` or `StringStore`.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab8096",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147df69",
   "metadata": {},
   "source": [
    "### Creating a Doc\n",
    "\n",
    "Let's create some Doc objects from scratch! The nlp object has already been created for you.\n",
    "\n",
    "By the way, if you haven't downloaded it already, check out the spaCy Cheat Sheet. It includes an overview of the most important concepts and methods and might come in handy if you ever need a quick refresher!\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "Import the Doc from spacy.tokens.\n",
    "Create a Doc from the words and spaces. Don't forget to pass in the vocab!\n",
    "\n",
    "Take Hint (-10 XP)\n",
    "2\n",
    "Import the Doc from spacy.tokens.\n",
    "Create a Doc from the words and spaces. Don't forget to pass in the vocab!\n",
    "3\n",
    "Import the Doc from spacy.tokens.\n",
    "Complete the words and spaces to match the desired text and create a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80cce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = ['spaCy', 'is', 'cool', '!']\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91a0354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = ['Go', ',', 'get', 'started', '!']\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb6dba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = ['Oh', ',', 'really', '?', '!']\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83029f9c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88721c4",
   "metadata": {},
   "source": [
    "### Docs, spans and entities from scratch\n",
    "\n",
    "In this exercise, you'll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "Import the Doc and Span classes from spacy.tokens.\n",
    "Use the Doc class directly to create a doc from the words and spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7705a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = ['I', 'like', 'David', 'Bowie']\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words = words, spaces = spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abb06d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie PERSON\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "print(span.text, span.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "716d0742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34d7a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf1c5a",
   "metadata": {},
   "source": [
    "### Data structures best practices\n",
    "\n",
    "The code in this example is trying to analyze a text and collect all proper nouns. If the token following the proper noun is a verb, it should also be extracted. A doc object has already been created.\n",
    "\n",
    "```python\n",
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == 'VERB':\n",
    "            print('Found a verb after a proper noun!')\n",
    "```\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Question\n",
    "Why is the code bad?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- The tokens in the result should be converted back to Token objects. This will let you reuse them in spaCy.\n",
    "- It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships.\n",
    "- pos_ is the wrong attribute to use for extracting proper nouns. You should use tag_ and the 'NNP' and 'NNS' labels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70f1712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'Berlin is a nice city'\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dba82c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47493446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "# as you can see it is a list of positional tags.\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bda7a",
   "metadata": {},
   "source": [
    "Re-write it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a4b5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a verb after a proper noun!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ in ('VERB', 'AUX'): # somehow 'be' is categorized as AUX contrary to class example\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c35694",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b8a2f",
   "metadata": {},
   "source": [
    "### Inspecting word vectors\n",
    "\n",
    "In this exercise, you'll use a larger English model, which includes around 20.000 word vectors. Because vectors take a little longer to load, we're using a slightly compressed version of it than the one you can download with spaCy. The model is already pre-installed, and spacy has already been imported for you.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Load the medium 'en_core_web_md' model with word vectors.\n",
    "Print the vector for \"bananas\" using the token.vector attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26a26004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.22.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (62.3.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8aa1a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
      " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
      " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
      " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
      "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
      " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
      "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
      " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
      "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
      "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
      "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
      " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
      "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
      " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
      " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
      "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
      " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
      "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
      "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
      " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
      " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
      "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
      "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
      " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
      " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
      "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
      " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
      "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
      "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
      " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
      "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
      " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
      "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
      " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
      "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
      " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
      " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
      "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
      " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
      " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
      " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
      " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
      " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
      "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
      " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
      " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
      "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
      " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
      " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
      " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_md model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2de16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d25d1",
   "metadata": {},
   "source": [
    "### Comparing similarities\n",
    "\n",
    "In this exercise, you'll be using spaCy's similarity methods to compare Doc, Token and Span objects and get similarity scores. The medium English model is already available as the nlp object.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "- Use the doc.similarity method to compare doc1 to doc2 and print the result.\n",
    "- Use the token.similarity method to compare token1 to token2 and print the result.\n",
    "- Create spans for \"great restaurant\"/\"really nice bar\".\n",
    "- Use span.similarity to compare them and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fda6a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8456855743724329\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "495fa6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18317238986492157\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\" \n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc5465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7541285157203674\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[-4:-1]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419071e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d88591",
   "metadata": {},
   "source": [
    "### Debugging patterns (1)\n",
    "\n",
    "Why does this pattern not match the tokens \"Silicon Valley\" in the doc?\n",
    "\n",
    "`pattern = [{'LOWER': 'silicon'}, {'TEXT': ' '}, {'LOWER': 'valley'}]`\n",
    "\n",
    "`doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")`\n",
    "\n",
    "You can try it out in your IPython shell. The matcher with the added pattern and the doc are already created.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "The tokens \"Silicon\" and \"Valley\" are not lowercase, so the 'LOWER' attribute won't match.\n",
    "\n",
    "The tokenizer doesn't create tokens for single spaces, so there's no token with the value ' ' in between.\n",
    "\n",
    "The tokens are missing an operator 'OP' to indicate that they should be matched exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66d347ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 0\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n",
    "\n",
    "pattern = [{'LOWER': 'silicon'}, {'TEXT': ' '}, {'LOWER': 'valley'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', [pattern], on_match = None)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bdf0bf",
   "metadata": {},
   "source": [
    "This does not print anything at all. Maybe it is `LOWER` attribute. We will test this now.\n",
    "\n",
    "The `LOWER` attribute in the pattern describes tokens whose lowercase form matches a given value. So {'LOWER': 'valley'} will match tokens like \"Valley\", \"VALLEY\", \"valley\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42c64e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 1\n",
      "Match found: Silicon Valley\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n",
    "\n",
    "pattern = [{'LOWER': 'silicon'}, {'LOWER': 'valley'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', [pattern], on_match = None)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed553a",
   "metadata": {},
   "source": [
    "The tokenizer already takes care of splitting off whitespace and each dictionary in the pattern describes one token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6011be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157538f4",
   "metadata": {},
   "source": [
    "### Debugging patterns (2)\n",
    "\n",
    "Both patterns in this exercise contain mistakes and won't match as expected. Can you fix them?\n",
    "\n",
    "The nlp and a doc have already been created for you. If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff693a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize Matcher() again\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"Twitch Prime, the perks program for Amazon Prime members offering free loot, \\\n",
    "games and other benefits, is ditching one of its best features: ad-free viewing. According \\\n",
    "to an email sent out to Amazon Prime members today, ad-free viewing will no longer be included \\\n",
    "as a part of Twitch Prime for new members, beginning on September 14. However, members \\\n",
    "with existing annual subscriptions will be able to continue to enjoy ad-free viewing until \\\n",
    "their subscription comes up for renewal. Those with monthly subscriptions will have access to \\\n",
    "ad-free viewing until October 15.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc53a14f",
   "metadata": {},
   "source": [
    "Original code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "165459e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'Amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad-free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', [pattern1], on_match = None)\n",
    "matcher.add('PATTERN2', [pattern2], on_match = None)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2745c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "# our fix is below\n",
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', [pattern1], on_match = None)\n",
    "matcher.add('PATTERN2', [pattern2], on_match = None)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb9b97",
   "metadata": {},
   "source": [
    "Why do we have to separate `ad-free` into three parts for Spacy `Matcher`. It needs more digging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aac0b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4b9a2",
   "metadata": {},
   "source": [
    "### Efficient phrase matching\n",
    "\n",
    "Sometimes it's more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world.\n",
    "\n",
    "We already have a list of countries, so let's use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES. The nlp object and a test doc have already been created and the doc.text has been printed to the shell.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
    "Add the phrase patterns and call the matcher on the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d21eca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the countries from COUNTRIES.txt\n",
    "with open('data/COUNTRIES.txt', 'r') as f:\n",
    "    COUNTRIES = [c.rstrip('\\n') for c in f.readlines()]\n",
    "text = 'Czech Republic may help Slovakia protect its airspace'\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8503b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "\n",
    "# patterns = [nlp.make_doc(country) for country in COUNTRIES]\n",
    "\n",
    "matcher.add('COUNTRY', patterns, on_match = None)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc504e3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73838810",
   "metadata": {},
   "source": [
    "### Extracting countries and relationships\n",
    "\n",
    "In the previous exercise, you wrote a script using spaCy's PhraseMatcher to find country names in text. Let's use that country matcher on a longer text, analyze the syntax and update the document's entities with the matched countries. The nlp object has already been created.\n",
    "\n",
    "The text is available as the variable text, the PhraseMatcher with the country patterns as the variable matcher. The Span class has already been imported.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "- Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n",
    "- Overwrite the entities in doc.ents and add the matched span.\n",
    "\n",
    "- Update the script and get the matched span's root head token.\n",
    "- Print the text of the head token and the span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "004c1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "with open ('data/long_text.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abbc4844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('US', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Yugoslavia', 'GPE'), ('Somalia', 'GPE'), ('US', 'GPE'), ('Mogadishu', 'GPE'), ('Bosnia', 'GPE'), ('Rwanda', 'GPE'), ('US', 'GPE'), ('Britain', 'GPE'), ('Singapore', 'GPE'), ('the United States', 'GPE'), ('Afghanistan', 'GPE'), ('the United States', 'GPE'), ('Iraq', 'GPE'), ('Darfur', 'GPE'), ('Sudan', 'GPE'), ('the Democratic Republic of Congo', 'GPE'), ('Haiti', 'GPE'), ('UN\\\\', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "    print(span)\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "507240a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\" and overwrite the doc.ents\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, '-->', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32832403",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4e5de",
   "metadata": {},
   "source": [
    "# Chapter 3: Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd5799",
   "metadata": {},
   "source": [
    "### What happens when you call nlp?\n",
    "\n",
    "What does spaCy do when you call nlp on a string of text? The IPython shell has a pre-loaded nlp object that logs what's going on under the hood. Try processing a text with it!\n",
    "\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Run the tagger, parser and entity recognizer and then the tokenizer.\n",
    "\n",
    "**Tokenize the text and apply each pipeline component in order.**\n",
    "\n",
    "Connect to the spaCy server to compute the result and return it.\n",
    "\n",
    "Initialize the language, add the pipeline and load in the binary model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "deacb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2320d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x16cde46a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x16cde47c0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x16cdc2ab0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x16ae5cd40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x169f48a00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x16cdc3220>)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a48fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2d3c0e",
   "metadata": {},
   "source": [
    "### Inspecting the pipeline\n",
    "\n",
    "Let's inspect the small English model's pipeline!\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Load the en_core_web_sm model and create the nlp object.\n",
    "- Print the names of the pipeline components using nlp.pipe_names.\n",
    "- Print the full pipeline of (name, component) tuples using nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a151b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "\n",
      "\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x16bef7700>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x16bef70a0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x16f68eff0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x170178fc0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x1701a3100>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x16f68ec00>)]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ae77c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241af61",
   "metadata": {},
   "source": [
    "### Use cases for custom components\n",
    "\n",
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. updating the pre-trained models and improving their predictions\n",
    "2. computing your own values based on tokens and their attributes\n",
    "3. adding named entities, for example based on a dictionary\n",
    "4. implementing support for an additional language\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "- 1 and 2.\n",
    "- 1 and 3.\n",
    "- 1 and 4.\n",
    "- **2 and 3.**\n",
    "- 2 and 4.\n",
    "- 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2070e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c3495",
   "metadata": {},
   "source": [
    "### Simple components\n",
    "\n",
    "The example shows a custom component that prints the character length of a document. Can you complete it? spacy has already been imported for you.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "- Complete the component function with the doc's length.\n",
    "- Add the length_component to the existing pipeline as the first component.\n",
    "- Try out the new pipeline and process any text with the nlp object – for example \"This is a sentence.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efcd1a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component('length_component')\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "  \n",
    "# Load the small English model and Add the component first in the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('length_component', first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp('This is a sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44892d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc9554",
   "metadata": {},
   "source": [
    "### Complex components\n",
    "\n",
    "In this exercise, you'll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents.\n",
    "\n",
    "A PhraseMatcher with the animal patterns has already been created as the variable matcher. The small English model is available as the variable nlp. The Span object has already been imported for you.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "Define the custom component and apply the matcher to the doc.\n",
    "Create a Span for each match, assign the label ID for 'ANIMAL' and overwrite the doc.ents with the new spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c626006",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_patterns = ['Golden Retriever', 'cat', 'turtle', 'Rattus norvegicus']\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "patterns = list(nlp.pipe(animal_patterns))\n",
    "\n",
    "matcher.add('ANIMALS', patterns, on_match = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19531fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "@Language.component('animal_component')\n",
    "def animal_component(doc):\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    # and overwrite the doc.ents with the matched spans\n",
    "    doc.ents = [Span(doc, start, end, label='ANIMAL')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "    \n",
    "# Add the component to the pipeline after the 'ner' component \n",
    "nlp.add_pipe('animal_component', after='ner')\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebef4af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77087f",
   "metadata": {},
   "source": [
    "### Setting extension attributes (1)\n",
    "\n",
    "Let's practice setting some extension attributes. The nlp object has already been created for you and the Doc, Token and Span classes are already imported.\n",
    "\n",
    "Remember that if you run your code more than once, you might see an error message that the extension already exists. That's because DataCamp will re-run your code in the same session. To solve this, you can set force=True on set_extension, or reload to start a new Python session. None of this will affect the answer you submit.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Use Token.set_extension to register is_country (default False).\n",
    "Update it for \"Spain\" and print it for all tokens.\n",
    "\n",
    "2\n",
    "\n",
    "Use Token.set_extension to register 'reversed' (getter function get_reversed).\n",
    "Print its value for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e86604b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension('is_country', default=False, force = True)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69609c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "  \n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef92ba3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c69028",
   "metadata": {},
   "source": [
    "### Setting extension attributes (2)\n",
    "\n",
    "Let's try setting some more complex attributes using getters and method extensions. The nlp object has already been created for you and the Doc, Token and Span classes are already imported.\n",
    "\n",
    "Remember that if you run your code more than once, you might see an error message that the extension already exists. That's because DataCamp will re-run your code in the same session. To solve this, you can set force=True on set_extension, or reload to start a new Python session. None of this will affect the answer you submit.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Complete the has_number function .\n",
    "Use Doc.set_extension to register 'has_number' (getter get_has_number) and print its value.\n",
    "\n",
    "2\n",
    "\n",
    "Use Span.set_extension to register 'to_html' (method to_html).\n",
    "Call it on doc[0:2] with the tag 'strong'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "202cfc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90d22a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension('to_html', method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(tag='strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3894d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320d050",
   "metadata": {},
   "source": [
    "### Entities and extensions\n",
    "\n",
    "In this exercise, you'll combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n",
    "\n",
    "The Span class is already imported and the nlp object has been created for you.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Complete the get_wikipedia_url getter so it only returns the URL if the span's label is in the list of labels.\n",
    "Set the Span extension 'wikipedia_url' using the getter get_wikipedia_url.\n",
    "Iterate over the entities in the doc and output their Wikipedia URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72a06cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url, force = True)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, \\\n",
    "David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cf195",
   "metadata": {},
   "source": [
    "This can be useful to create automated links for certain entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13615f0e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9522321",
   "metadata": {},
   "source": [
    "### Components with extensions\n",
    "\n",
    "Extension attributes are especially powerful if they're combined with custom pipeline components. In this exercise, you'll write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available.\n",
    "\n",
    "The nlp object has already been created and the Span class is already imported. A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable capitals.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "- Complete the countries_component and create a Span with the label 'GPE' (geopolitical entity) for all matches.\n",
    "- Add the component to the pipeline.\n",
    "- Register the Span extension attribute 'capital' with the getter get_capital.\n",
    "- Process the text and print the entity text, entity label and entity capital for each entity span in doc.ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f24c985",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read the capitals dictionary from json file\n",
    "import json\n",
    "\n",
    "with open('data/capitals.json', 'r') as f:\n",
    "    capitals = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9abbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "\n",
    "matcher.add('COUNTRY', patterns, on_match = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8998f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "@Language.component('countries_component')\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label='GPE')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe('countries_component')\n",
    "\n",
    "# Register capital and getter that looks up the span text in country capitals\n",
    "Span.set_extension('capital', getter=lambda span: capitals.get(span.text))\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df201e",
   "metadata": {},
   "source": [
    "One of the problems I have encountered is forgetting to create a `Matcher` with countries list has been appended. If you try to replicate DataCamp, this is something that you will be missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15ff75",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8ff22",
   "metadata": {},
   "source": [
    "### Processing streams\n",
    "\n",
    "In this exercise, you'll be using nlp.pipe for more efficient text processing. The nlp object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable TEXTS.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "- Rewrite the example to use nlp.pipe. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe.\n",
    "- Rewrite the example to use nlp.pipe. Don't forget to call list() around the result to turn it into a list.\n",
    "- Rewrite the example to use nlp.pipe. Don't forget to call list() around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04fd043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = ['McDonalds is my favorite restaurant.',\n",
    " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    " 'People really still eat McDonalds :(',\n",
    " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aed58509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open']\n",
      "['terrible', 'payin']\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42dfa564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() () () (Spain,) () () ()\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0005a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175495fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528ac2b",
   "metadata": {},
   "source": [
    "### Processing data with context\n",
    "\n",
    "In this exercise, you'll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of (text, context) examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys 'author' and 'book'. The nlp object has already been created for you.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Import the Doc class and use the set_extension method to register the custom attributes 'author' and 'book', which default to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "154cbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
    "  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}),\n",
    " (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}),\n",
    " ('It was the best of times, it was the worst of times.',\n",
    "  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}),\n",
    " ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n",
    "  {'author': 'Jack Kerouac', 'book': 'On the Road'}),\n",
    " ('It was a bright cold day in April, and the clocks were striking thirteen.',\n",
    "  {'author': 'George Orwell', 'book': '1984'}),\n",
    " ('Nowadays people know the price of everything and the value of nothing.',\n",
    "  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffdaf817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class and register the extensions 'author' and 'book'\n",
    "from spacy.tokens import Doc\n",
    "Doc.set_extension('book', default=None)\n",
    "Doc.set_extension('author', default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples = True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d008784",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77752d15",
   "metadata": {},
   "source": [
    "### Selective processing\n",
    "\n",
    "In this exercise, you'll use the nlp.make_doc and nlp.disable_pipes methods to only run selected components when processing a text. The small English model is already loaded in as the nlp object.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "- Rewrite the code to only tokenize the text using nlp.make_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5b7ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered \\\n",
    "in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d370a879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Georgia,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmustafaicer/opt/anaconda3/envs/spacy/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the \\\n",
    "city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10a672",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6edbf05",
   "metadata": {},
   "source": [
    "# Chapter 4: Training a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181e9c3",
   "metadata": {},
   "source": [
    "### Purpose of training\n",
    "\n",
    "While spaCy comes with a range of pre-trained models to predict linguistic annotations, you almost always want to fine-tune them with more examples. You can do this by training them with more labelled data.\n",
    "\n",
    "What does training not help with?\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "- Improve model accuracy on your data.\n",
    "- Learn new classification schemes.\n",
    "- Discover patterns in unlabelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a2faf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ddee01",
   "metadata": {},
   "source": [
    "### Creating training data (1)\n",
    "\n",
    "spaCy's rule-based Matcher is a great way to quickly create training data for named entity models. A list of sentences is available as the variable TEXTS. You can print it the IPython shell to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as 'GADGET'.\n",
    "\n",
    "The nlp object has already been created for you and the Matcher is available as the variable matcher.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Write a pattern for two tokens whose lowercase forms match 'iphone' and 'x'.\n",
    "- Write a pattern for two tokens: one token whose lowercase form matches 'iphone' and an optional digit using the '?' operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a62d9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = ['How to preorder the iPhone X',\n",
    " 'iPhone X is coming',\n",
    " 'Should I pay $1,000 for the iPhone X?',\n",
    " 'The iPhone 8 reviews are here',\n",
    " 'Your iPhone goes up to 11 today',\n",
    " 'I need a new phone! Any tips?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc7f01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '+'}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('GADGET', [pattern1, pattern2], on_match = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a219d6",
   "metadata": {},
   "source": [
    "If I change `'OP'` from `+` to `?` it catches multiple cases for spans. Therefore, it creates overlapping entities which cause an error in the following exercises:\n",
    "\n",
    "```\n",
    "ValueError: [E103] Trying to set conflicting doc.ents: '(4, 10, 'GADGET')' and '(4, 12, 'GADGET')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead.\n",
    "```\n",
    "\n",
    "Therefore, I used `+` which is >>> Require the pattern to match 1 or more times.\n",
    "\n",
    "See [this](https://spacy.io/usage/rule-based-matching#quantifiers) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5bc6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872141b6",
   "metadata": {},
   "source": [
    "### Creating training data (2)\n",
    "\n",
    "Let's use the match patterns we've created in the previous exercise to bootstrap a set of training examples. The nlp object has already been created for you and the Matcher with the added patterns pattern1 and pattern2 is available as the variable matcher. A list of sentences is available as the variable TEXTS.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "- Create a doc object for each text using nlp.pipe and find the matches in it.\n",
    "- Create a list of (start, end, label) tuples for the matches.\n",
    "\n",
    "- Match on the doc and create a list of matched spans.\n",
    "- Format each example as a tuple of the text and a dict, mapping 'entities' to the entity tuples.\n",
    "- Append the example to TRAINING_DATA and inspect the printed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6da7cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Doc object for each text in TEXTS\n",
    "# for doc in nlp.pipe(TEXTS):\n",
    "#     # Find the matches in the doc\n",
    "#     matches = matcher(doc)\n",
    "    \n",
    "#     # Get a list of (start, end, label) tuples of matches in the text\n",
    "#     entities = [(start, end, 'GADGET') for match_id, start, end in matches]\n",
    "#     print(doc.text, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6df7829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': []})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "print(*TRAINING_DATA, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2e9e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0b0a2",
   "metadata": {},
   "source": [
    "### Setting up the pipeline\n",
    "\n",
    "In this exercise, you'll prepare a spaCy pipeline to train the entity recognizer to recognize 'GADGET' entities in a text – for exampe, \"iPhone X\".\n",
    "\n",
    "spacy has already been imported for you.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "- Create a blank 'en' model, for example using the spacy.blank method.\n",
    "- Create a new entity recognizer using nlp.create_pipe and add it to the pipeline.\n",
    "- Add the new label 'GADGET' to the entity recognizer using the add_label method on the pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2fdcf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.add_pipe('ner')\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191fb6e5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614eab3",
   "metadata": {},
   "source": [
    "### Building a training loop\n",
    "\n",
    "Let's write a simple training loop from scratch!\n",
    "\n",
    "The pipeline you've created in the previous exercise is available as the nlp object. It already contains the entity recognizer with the added label 'GADGET'.\n",
    "\n",
    "The small set of labelled examples that you've created previously is available as the global variable TRAINING_DATA. To see the examples, you can print them in your script or in the IPython shell. spacy and random have already been imported for you.\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "- Call nlp.begin_training, create a training loop for 10 iterations and shuffle the training data.\n",
    "- Create batches of training data using spacy.util.minibatch and iterate over the batches.\n",
    "- Convert the (text, annotations) tuples in the batch to lists of texts and annotations.\n",
    "- For each batch, use nlp.update to update the model with the texts and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39b4250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e467ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 3.3333334922790527}\n",
      "{'ner': 8.180318057537079}\n",
      "{'ner': 13.557176649570465}\n",
      "{'ner': 20.873930156230927}\n",
      "{'ner': 27.07882982492447}\n",
      "{'ner': 30.862092196941376}\n",
      "{'ner': 3.1524229645729065}\n",
      "{'ner': 5.1253800094127655}\n",
      "{'ner': 7.442873924970627}\n",
      "{'ner': 9.033771596848965}\n",
      "{'ner': 9.566545123234391}\n",
      "{'ner': 10.793928442522883}\n",
      "{'ner': 0.8937611355213448}\n",
      "{'ner': 0.9122877491463441}\n",
      "{'ner': 0.9184966012253426}\n",
      "{'ner': 1.6712227741008974}\n",
      "{'ner': 4.0508636953085215}\n",
      "{'ner': 6.332585643911557}\n",
      "{'ner': 1.8881516805537188}\n",
      "{'ner': 1.8881595654577978}\n",
      "{'ner': 1.8882107583402816}\n",
      "{'ner': 2.8234261669064065}\n",
      "{'ner': 3.4541588698815726}\n",
      "{'ner': 4.205019054624225}\n",
      "{'ner': 4.551585658926771e-05}\n",
      "{'ner': 0.24552291906176235}\n",
      "{'ner': 0.451009931903815}\n",
      "{'ner': 0.48081180547336855}\n",
      "{'ner': 0.4846418308257415}\n",
      "{'ner': 0.48464183420350176}\n",
      "{'ner': 9.40943683719591e-06}\n",
      "{'ner': 3.735847416379192e-05}\n",
      "{'ner': 4.8114050091441554e-05}\n",
      "{'ner': 5.330914049146186e-05}\n",
      "{'ner': 5.331062105000568e-05}\n",
      "{'ner': 5.366658920998198e-05}\n",
      "{'ner': 1.0100926612954851e-07}\n",
      "{'ner': 1.1821601916724872e-05}\n",
      "{'ner': 1.2033229383520369e-05}\n",
      "{'ner': 1.2111670583472709e-05}\n",
      "{'ner': 1.214170837412595e-05}\n",
      "{'ner': 1.2142156768319036e-05}\n",
      "{'ner': 3.016814649678824e-06}\n",
      "{'ner': 3.0349545412298423e-06}\n",
      "{'ner': 3.0355323707357653e-06}\n",
      "{'ner': 3.1129001146334507e-06}\n",
      "{'ner': 3.1481471312350774e-06}\n",
      "{'ner': 3.1516511533457393e-06}\n",
      "{'ner': 1.0257384371449703e-08}\n",
      "{'ner': 1.1364469974988539e-08}\n",
      "{'ner': 1.1753463059314288e-08}\n",
      "{'ner': 2.0708926381778285e-07}\n",
      "{'ner': 2.08087465578832e-07}\n",
      "{'ner': 3.198776645772769e-07}\n",
      "{'ner': 2.0754389127978536e-10}\n",
      "{'ner': 4.052619679997902e-10}\n",
      "{'ner': 1.0036062633334646e-09}\n",
      "{'ner': 3.4381520817202064e-09}\n",
      "{'ner': 3.0258603509817506e-08}\n",
      "{'ner': 1.3425523901306064e-07}\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        for text, annotations in batch:\n",
    "            # create Example\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "        \n",
    "            # Update the model\n",
    "            nlp.update([example], losses=losses)\n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc8f5b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e9130",
   "metadata": {},
   "source": [
    "### Exploring the model\n",
    "\n",
    "Let's see how the model performs on unseen data! To speed things up a little, here's a trained model for the label 'GADGET', using the examples from the previous exercise, plus a few hundred more. The loaded model is already available as the nlp object. A list of test texts is available as TEST_DATA.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "- Process each text in TEST_DATA using nlp.pipe.\n",
    "- Print the document text and the entities in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5957e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = ['Apple is slowing down the iPhone 8 and iPhone X - how to stop it',\n",
    " \"I finally understand what the iPhone X 'notch' is for\",\n",
    " 'Everything you need to know about the Samsung Galaxy S9',\n",
    " 'Looking to compare iPad models? Here’s how the 2018 lineup stacks up',\n",
    " 'The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple',\n",
    " 'what is the cheapest ipad, especially ipad pro???',\n",
    " 'Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43220376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is slowing down the iPhone 8 and iPhone X - how to stop it\n",
      "(iPhone 8, iPhone X) \n",
      "\n",
      "\n",
      "I finally understand what the iPhone X 'notch' is for\n",
      "(iPhone X,) \n",
      "\n",
      "\n",
      "Everything you need to know about the Samsung Galaxy S9\n",
      "() \n",
      "\n",
      "\n",
      "Looking to compare iPad models? Here’s how the 2018 lineup stacks up\n",
      "() \n",
      "\n",
      "\n",
      "The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\n",
      "(iPhone 8, iPhone 8) \n",
      "\n",
      "\n",
      "what is the cheapest ipad, especially ipad pro???\n",
      "() \n",
      "\n",
      "\n",
      "Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\n",
      "() \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each text in TEST_DATA\n",
    "for doc in nlp.pipe(TEST_DATA):\n",
    "    # Print the document text and entitites\n",
    "    print(doc.text)\n",
    "    print(doc.ents, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60178b3c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017534bf",
   "metadata": {},
   "source": [
    "### Good data vs. bad data\n",
    "\n",
    "Here's an excerpt from a training set that labels the entity type TOURIST_DESTINATION in traveler reviews.\n",
    "\n",
    "#### Instructions 1/2\n",
    "\n",
    "Question\n",
    "Why is this data and label scheme problematic?\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "- **Whether a place is a tourist destination is a subjective judgement and not a definitive category. It will be very difficult for the entity recognizer to learn.**\n",
    "\n",
    "- \"Paris\" and \"Arkansas\" should also be labelled as tourist destinations for consistency. Otherwise, the model will be confused.\n",
    "\n",
    "- Rare out-of-vocabulary words like the misspelled \"amsterdem\" shouldn't be labelled as entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7f4d2",
   "metadata": {},
   "source": [
    "```python\n",
    "('i went to amsterdem last year and the canals were beautiful', {'entities': [(10, 19, 'TOURIST_DESTINATION')]})\n",
    "\n",
    "('You should visit Paris once in your life, but the Eiffel Tower is kinda boring', {'entities': [(17, 22, 'TOURIST_DESTINATION')]})\n",
    "\n",
    "(\"There's also a Paris in Arkansas, lol\", {'entities': []})\n",
    "\n",
    "('Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!', {'entities': [(0, 6, 'TOURIST_DESTINATION')]})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6b1df",
   "metadata": {},
   "source": [
    "- Rewrite the TRAINING_DATA to only use the label GPE (cities, states, countries) instead of TOURIST_DESTINATION.\n",
    "- Don't forget to add tuples for the GPE entities that weren't labeled in the old data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78f482ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i went to amsterdem last year and the canals were beautiful', {'entities': [(10, 19, 'GPE')]})\n",
      "('You should visit Paris once in your life, but the Eiffel Tower is kinda boring', {'entities': [(17, 22, 'GPE')]})\n",
      "(\"There's also a Paris in Arkansas, lol\", {'entities': [(15, 20, 'GPE'), (24, 32, 'GPE')]})\n",
      "('Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!', {'entities': [(0, 6, 'GPE')]})\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"i went to amsterdem last year and the canals were beautiful\", {'entities': [(10, 19, 'GPE')]}),\n",
    "    (\"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\", {'entities': [(17, 22, 'GPE')]}),\n",
    "    (\"There's also a Paris in Arkansas, lol\", {'entities': [(15, 20, 'GPE'), (24, 32, 'GPE')]}),\n",
    "    (\"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\", {'entities': [(0, 6, 'GPE')]})\n",
    "]\n",
    "     \n",
    "print(*TRAINING_DATA, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817846a",
   "metadata": {},
   "source": [
    "Great work! Once the model achieves good results on detecting `GPE` entities in the traveler reviews, you could add a rule-based component to determine whether the entity is a tourist destination in this context. For example, you could resolve the entities types back to a knowledge base or look them up in a travel wiki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c838f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc890fb1",
   "metadata": {},
   "source": [
    "### Training multiple labels\n",
    "\n",
    "Here's a small sample of a dataset created to train a new entity type WEBSITE. The original dataset contains a few thousand sentences. In this exercise, you'll be doing the labeling by hand. In real life, you probably want to automate this and use an annotation tool – for example, [Brat](http://brat.nlplab.org/), a popular open-source solution, or [Prodigy](https://prodi.gy/), our own annotation tool that integrates with spaCy.\n",
    "\n",
    "After this exercise you will be nearly done with the course! If you enjoyed it, feel free to send Ines a thank you via Twitter - she'll appreciate it! Tweet to Ines\n",
    "\n",
    "#### Instructions 1/3\n",
    "\n",
    "- Complete the entity offsets for the WEBSITE entities in the data. Feel free to use len() if you don't want to count the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7108d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"Reddit partners with Patreon to help creators build communities\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {'entities': [(18, 25, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n",
    "     {'entities': [(0, 6, 'WEBSITE')]}),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55230792",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "A model was trained with the data you just labelled, plus a few thousand similar examples. After training, it's doing great on WEBSITE, but doesn't recognize PERSON anymore. Why could this be happening?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- It's very difficult for the model to learn about different categories like PERSON and WEBSITE.\n",
    "- **The training data included no examples of PERSON, so the model learned that this label is incorrect.**\n",
    "- The hyperparameters need to be retuned so that both entity types can be recognized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510275fa",
   "metadata": {},
   "source": [
    "- Update the training data to include annotations for the PERSON entities \"PewDiePie\" and \"Alexis Ohanian\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3681cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"Reddit partners with Patreon to help creators build communities\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {'entities': [(0, 9, 'PERSON'), (18, 25, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (15, 29, 'PERSON')]}),\n",
    "    # And so on...\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
